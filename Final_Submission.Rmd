---
title: "Twitter Sentiment Analysis Kaggle Competition"
subtitle: "The Analytical Edge"
author: Samuel Sim Wei Xuan, Lee Min Shuen
date: "Term 6"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
    number_sections: true
---

<style>
body {
text-align: justify}
</style>

# Preparation 

Preparing environment and loading datasets.

```{r message=FALSE, warning=FALSE}
# Clear Environment -------------------------------------------------------
rm(list=ls()) 

# Libraries ---------------------------------------------------------------

library(tidyverse) # To load dplyr, ggplot2
library(caTools) # For sample.split
library(keras) # To load the neural network package
library(reticulate) # To enable python calling when using Keras
library(randomForest) # For Random Forest
library(e1071) # For Naive Bayes Classification
library(rpart) # For Classification Tree
library(pROC) # For multiclass ROC
library(tm) # To load text mining package
library(textclean) # To load text cleaning package
library(fastDummies) # One-hot encoding
library(deepviz) # For RNN/LSTM vis

# Import ------------------------------------------------------------------
train <- read.csv("Data/train.csv", stringsAsFactors=FALSE)
test <- read.csv("Data/test.csv", stringsAsFactors=FALSE)
```

# Probabilisitc Approach

## Preparing Data

```{r message=FALSE, warning=FALSE}
# Read the Twitter data
train <- read.csv("Data/train.csv", stringsAsFactors = F)

# Preparing document term matrix using tm package
corpus <- Corpus(VectorSource(train$tweet))
corpus <- tm_map(corpus,content_transformer(tolower))
corpus <- tm_map(corpus,removeWords,stopwords("english"))
corpus <- tm_map(corpus,removePunctuation)
corpus <- tm_map(corpus,stemDocument)
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.995)

twittersparse <- as.data.frame(as.matrix(dtm))
colnames(twittersparse) <- make.names(colnames(twittersparse))
twittersparse$sentiment <- train$sentiment

# Check if data is imbalanced
table(train$sentiment)/nrow(train)

# Split on the labels
trainid <- sample.split(twittersparse$sentiment, 0.7)
twittersparse_train <- twittersparse[trainid,]
twittersparse_valid <- twittersparse[!trainid,]
```

## Classification Tree (with pruning)

```{r}
model_CT <- rpart(sentiment~.,data = twittersparse_train, method = "class",cp=10^-6)
opt <- which.min(model_CT$cptable[,"xerror"])
cp <- model_CT$cptable[opt, "CP"]
model_CT <- prune(model_CT,cp)

# model_CT <- saveRDS(model_CT,"model_CT.rds")
# model_CT <- readRDS("model_CT.rds")

predictclass1 <- predict(model_CT,newdata=twittersparse_valid,type="class")
confusionmatrix1  <- table(predictclass1,twittersparse_valid$sentiment) 
Accuracy1 <- sum(diag(confusionmatrix1))/sum(confusionmatrix1)
Accuracy1
```

## Random Forest (without grid parameter search)

```{r}
model_RF <- randomForest(as.factor(sentiment)~.,data=twittersparse_train)

# model_RF <- saveRDS(model_RF,"model_RF.rds")
# model_RF <- readRDS("model_RF.rds")

predictclass2 <- predict(model_RF,newdata=twittersparse_valid,type="class")
confusionmatrix2  <- table(predictclass2,twittersparse_valid$sentiment) 
Accuracy2 <- sum(diag(confusionmatrix2))/sum(confusionmatrix2)
Accuracy2
```

## Naive Bayes Classifier

```{r}
model_NB <- naiveBayes(as.factor(sentiment)~.,data=twittersparse_train)

# model_NB <- saveRDS(model_NB,"model_NB.rds")
# model_NB <- readRDS("model_NB.rds")

predictclass3 <- predict(model_NB,newdata=twittersparse_valid,type="class")
confusionmatrix3  <- table(predictclass3,twittersparse_valid$sentiment) 
Accuracy3 <- sum(diag(confusionmatrix3))/sum(confusionmatrix3)
Accuracy3
```

## AUC Values

```{r}
# Get the probabilities
predictprob1 <- predict(model_CT,newdata=twittersparse_valid,type="prob") # Classfication Tree
predictprob2 <- predict(model_RF,newdata=twittersparse_valid,type="prob") # Random Forest
predictprob3 <- predict(model_NB,newdata=twittersparse_valid,type="raw") # Naive Bayes

# Calculate AUCs
multiclass.roc(twittersparse_valid$sentiment, predictprob1)$auc
multiclass.roc(twittersparse_valid$sentiment, predictprob2)$auc
multiclass.roc(twittersparse_valid$sentiment, predictprob3)$auc
```

# Deep-learning Approach

## Preparing the data

We shall only change all the text to lowercase, replace drawn-out words with their correct spelling, replace emoticons with their associated emotion, and remove internet addresses. To do so, we will be using the `textclean` library.

```{r message=FALSE, warning=FALSE}
rm(list=ls()) 
train <- read.csv("Data/train.csv", stringsAsFactors=FALSE)
test <- read.csv("Data/test.csv", stringsAsFactors=FALSE)

train <- train %>% mutate(Split = "train")
test <- test %>% mutate(Split = "test")

# Combine data for tokenization
full <- data.frame(rbind(train %>% select(-sentiment), test %>% select(-id)))

# Process the Text
raw_text <- full$tweet

# Using textclean library
processed_text <- raw_text %>%
  tolower() %>%
  replace_word_elongation() %>%
  replace_internet_slang() %>%
  replace_emoticon() %>%
  replace_url() %>%
  replace_email() %>%
  replace_html()

full_processed <- full
full_processed$tweet <- processed_text
```

Tokenization Step

```{r message=FALSE, warning=FALSE}
# Maximum number of words to consider as features, our vocabulary size
max_words <- 15000

# Prepare to tokenize the text
texts <- full_processed$tweet

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

# Tokenize - i.e. splits text into a sequence of integer tokens
sequences <- texts_to_sequences(tokenizer, texts)
word_index <- tokenizer$word_index
```

As the observations of our data could have varying lengths, the final sequence is zero-padded so that each input sequence is of the same length. We shall use the maximum observed raw sequence length to keep things simple as the standard sequence length. We also pad the sequences. 

For any observation, if there are no words in the $i^{th}$ position of the sequence, it is simply represented as a token with value $0$.

```{r}
# Length of sequence
maxlen <- max(as.numeric(summary(sequences)[,1]))

# Pad out texts so everything is the same length
data <- pad_sequences(sequences, maxlen = maxlen)
```

Then we perform one-hot encoding of the train labels, as `Keras` requires data inputs to be of the matrix class. Note the test labels are not given in the Kaggle competition.

```{r}
# Split back into train and test
train_matrix <- data[1:nrow(train),]
test_matrix <- data[(nrow(train)+1):nrow(data),]

# One-hot encoding of train_labels
train_labels <- dummy_cols(data.frame(V=train$sentiment)) %>% select(everything(),-V)
```

Finally, we create a training and validation split for the training data.

```{r}
# Split into training and validation set
trainid <- sample.split(train$sentiment, 0.7)

x_train <- as.matrix(train_matrix[trainid,])
y_train <- as.matrix(train_labels[trainid,])

x_val <- as.matrix(train_matrix[!trainid,])
y_val <- as.matrix(train_labels[!trainid,])
```

## Simple RNN model

```{r message=FALSE, warning=FALSE}
# Word Embedding 
embedding_dim <- 11 
# According to https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html
# Number of embedding dim = vocab_size**0.25

model1 <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, # Size of the vocabulary of the word embedding
                  output_dim = embedding_dim, # No. of features of each token 
                  input_length = maxlen) %>%  # No. of tokens to be created = input sequence length, number of timesteps
  layer_simple_rnn(units = 30, # n, size of the hidden layers (weight matrices)
                   return_sequences = FALSE) %>%
  layer_dense(units = 3, activation = "softmax")

summary(model1)
plot_model(model1)
```

Lets compile and fit the model!

```{r}
# Compile model
model1 %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.01),
  loss = "categorical_crossentropy",
  metrics = "categorical_accuracy"
)

history <- model1 %>% fit(
  x_train,
  y_train,
  batch_size = 64,
  validation_data = list(x_val, y_val),
  epochs = 100,
  view_metrics = FALSE,
  verbose = 0
)

print(history)
plot(history)

# Final training accuracy
round(history$metrics$categorical_accuracy[length(history$metrics$categorical_accuracy)],3)
# Final validation accuracy
round(history$metrics$val_categorical_accuracy[length(history$metrics$val_categorical_accuracy)],3)
```

We can observe that the performance of a simple RNN model on our sentiment analysis task is not very ideal, with a training accuracy of `r round(history$metrics$categorical_accuracy[length(history$metrics$categorical_accuracy)],3)` and validation accuracy of `r round(history$metrics$val_categorical_accuracy[length(history$metrics$val_categorical_accuracy)],3)`. This is actually due to the vanishing gradient problem that we discussed earlier! To see this, we will approach this task using a LSTM model instead, keeping all other parameters the same.

## Simple LSTM model

```{r}
# Word Embedding 
model2 <- keras_model_sequential()
model2 %>%
  layer_embedding(input_dim = max_words, 
                  output_dim = embedding_dim, 
                  input_length = maxlen) %>%  
  layer_lstm(units = 30,
             return_sequences = FALSE) %>%
  layer_dense(units = 3, activation = "softmax")

summary(model2)
plot_model(model2)
```

Lets compile and fit the LSTM model!

```{r}
# Compile model
model2 %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.01),
  loss = "categorical_crossentropy",
  metrics = "categorical_accuracy"
)

history2 <- model2 %>% fit(
  x_train,
  y_train,
  batch_size = 64,
  validation_data = list(x_val, y_val),
  epochs = 100,
  view_metrics = FALSE,
  verbose = 0
)

print(history2)
plot(history2)

# Final training accuracy
round(history2$metrics$categorical_accuracy[length(history2$metrics$categorical_accuracy)],3)
# Final validation accuracy
round(history2$metrics$val_categorical_accuracy[length(history2$metrics$val_categorical_accuracy)],3)
```

Just by switching from a simple RNN model to an LSTM model, where the main difference between the two implementations is that the issue of vanishing gradient is addressed, we can observe an increase in the model's performance. With the LSTM model, we achieve a training accuracy of `r round(history2$metrics$categorical_accuracy[length(history2$metrics$categorical_accuracy)],3)` and validation accuracy of `r round(history2$metrics$val_categorical_accuracy[length(history2$metrics$val_categorical_accuracy)],3)`.  Note the difference in training accuracy compared to our very first example of using a classification tree. A neural network model built on the concept of sequential data performs way better!

## Ways to improve performance

Despite the increase in model performance, it is evident that the model is over fitting to the training data. There could be many reasons for this, perhaps there are some under-represented words in the training data, or perhaps there was too little training data, etc. 

So how can we improve our performance?

* Increase generality of model
    + Add dropout and regularization
* Hyper-parameter tuning
    + Experiment with different hyper-parameter settings
* Data augmentation
    + Back Translation (Make the data more diverse by translating it to another language and back)
    + Easy Data Augmentation (Synonym replacement, Random insertion, Random swap, Random deletion)
    + Contextualized Word Embeddings (ELMo)
* Use more complex and state-of-the art models
    + Transfer learning (Pre-trained word embeddings)
    + Transformer-based models (BERT) 
    + Multitask Unified Model (MUM) 
    
## Complex LSTM model with Pre-trained Word Embeddings

The two pre-trained word embeddings are:

* GloVe Twitter 200 
* Fast Text Twitter 100

For the deep neural network, we added an additional dense layer of size 128 to increase the number of training parameters. Also for the respective LSTM layers, we shall increase the size to 64. To improve the over fitting we had, we shall include 2 dropout layers.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Pre-Trained Word Embedding
glove_twitter_embedding_dim <- 200
fasttext_twitter_embedding_dim <- 100
glove_twitter_weights <- readRDS("Word Embedding Weights/glove_twitter_27B_200d.rds") 
fasttext_twitter_weights <- readRDS("Word Embedding Weights/fasttext_english_twitter_100d.rds")

# Input layer
input <- layer_input(
  shape = list(NULL),
  dtype = "int32",
  name = "input"
)

# Glove Embedding and LSTM layer 1
encoded_1 <- input %>%
  layer_embedding(name = "glove_twitter_embedding",
                  input_dim = max_words, 
                  output_dim = glove_twitter_embedding_dim, 
                  input_length = maxlen) %>%  
  layer_lstm(units = 64, 
             input_shape = c(maxlen, glove_twitter_embedding_dim), 
             return_sequences = FALSE)

# Fasttext Embedding and LSTM layer 2
encoded_2 <- input %>% 
  layer_embedding(name = "fasttext_twitter_embedding",
                  input_dim = max_words, 
                  output_dim = fasttext_twitter_embedding_dim, 
                  input_length = maxlen) %>%  
  layer_lstm(units = 64, 
             input_shape = c(maxlen, fasttext_twitter_embedding_dim), 
             return_sequences = FALSE)

model3 <-keras_model(input,(
  layer_concatenate(list(encoded_1,encoded_2)) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 3, activation = "softmax")))

# Set the weights to the pretrained word embedding weights
get_layer(model3, name = "glove_twitter_embedding") %>%
  set_weights(list(glove_twitter_weights)) %>%
  freeze_weights()

get_layer(model3, name = "fasttext_twitter_embedding") %>%
  set_weights(list(fasttext_twitter_weights)) %>%
  freeze_weights()

summary(model3)
```

Lets compile and fit the new LSTM model! For our learning rate we shall use 0.001 instead of 0.01 to see if we get better results.

```{r}
# Compile model

model3 %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.001),
  loss = "categorical_crossentropy",
  metrics = "categorical_accuracy"
)

# Fit model 
history3 <- model3 %>% fit(
  x_train,
  y_train,
  batch_size = 64,
  validation_data = list(x_val, y_val),
  epochs = 100,
  view_metrics = FALSE,
  verbose = 0
)

print(history3)
plot(history3)
```

Now that we notice our model results plateauing ,let us use a even smaller learning rate to see if we can obtain even better results. This concept is called **adaptive learning rate**. In addition, we will unfreeze all the weights including the pretrained word embedding for small fine tuning.

```{r message=FALSE, warning=FALSE}
# Unfreeze weights
unfreeze_weights(model3, from = "input")

# Reduce LR and compile
model3 %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.0001),
  loss = "categorical_crossentropy",
  metrics = "categorical_accuracy"
)

# Train model briefly, 50 epochs instead of 100
history3b <- model3 %>% fit(
  x_train,
  y_train,
  batch_size = 64,
  validation_data = list(x_val, y_val),
  epochs = 50,
  view_metrics = FALSE,
  verbose = 0
)

# Look at training results
print(history3b)
plot(history3b)

# Final training accuracy
round(history3b$metrics$categorical_accuracy[length(history3b$metrics$categorical_accuracy)],3)
# Final validation accuracy
round(history3b$metrics$val_categorical_accuracy[length(history3b$metrics$val_categorical_accuracy)],3)
```
